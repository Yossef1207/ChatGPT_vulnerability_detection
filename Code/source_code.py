from tqdm import tqdm
import numpy as np
import tiktoken
import openai
import progressbar
import sys
import time
import json
import textwrap
import open_ai_related_functions

def print_generated_code(file, function_source):
    summary = open_ai_related_functions.summarize_code(function_source)
    file.write('GPT generated text: \n' + summary + '\n\n')
    return summary


if __name__ == '__main__':
    # json_file = '/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/dataset.json'
    output_file = '/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_3/test.txt'
    gpt_json_file = '/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_3/gpt_output.json'
    filtered_file = '/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/filtered_data.json'

    args = sys.argv
    limit = int(args[1])  # the limit of functions we want to have
    count = 1
    gpt_data = []

    """with open(json_file) as f:
        data = json.load(f)"""
    with open(filtered_file, "r") as f:
        filtered_data = json.load(f)

    token_error = "Too many tokens, max is 4096 \n"

    """filtered_data = []
    for d in data:
        if all(key in d for key in ['cwe_id', 'cwe_name', 'method_before', 'method_after']):
            if d['cwe_id'] != "Not Mapping" and d['cwe_name'] != "Not Mapping":
                filtered_data.append(d)"""

    with open(output_file, "w") as file:
        with tqdm(total=limit) as pbar:
            for d in filtered_data:
                file.write('Case: ' + str(count) + '\n\n')
                file.write('cwe_id: ' + d['cwe_id'] + '\n\n')
                file.write('cwe_name: ' + d['cwe_name'] + '\n\n')
                file.write('vuln_id: ' + d['vuln_id'] + '\n\n')
                file.write('desc: ' + d['desc'] + '\n\n')

                file.write('\n----------- This is the vulnerable version ' + ' ( case: ' + str(count) + ') -----------\n\n')
                formatted_string = textwrap.dedent(d['method_before']).strip()
                file.write(formatted_string + '\n\n')
                gen_summery_before = print_generated_code(file, d['method_before'])
                file.write('\n----------- This is the fixed version ' + ' ( case: ' + str(count) + ') -----------\n\n')
                formatted_string = textwrap.dedent(d['method_after']).strip()
                file.write(formatted_string + '\n\n')
                gen_summery_after = print_generated_code(file, d['method_after'])
                file.write('\n\n\n\n')
                if gen_summery_before != token_error and gen_summery_after != token_error:
                    count += 1
                    new_gpt_data = {'idx': d['idx'],
                                    'vuln_id': d['vuln_id'],
                                    'desc: ': d['desc'],
                                    'cwe_id': d['cwe_id'],
                                    'cwe_name': d['cwe_name'],
                                    'repo': d['repo'],
                                    'commit': d['commit'],
                                    'method_before': d['method_before'],
                                    'gpt_review_before': gen_summery_before,
                                    'method_after': d['method_after'],
                                    'gpt_review_after': gen_summery_after,
                                    }
                    gpt_data.append(new_gpt_data)
                    pbar.update(1)
                if count > limit:
                    break
        file.write('The output is printed in: \n' + output_file + '\n')

    with open(gpt_json_file, "w") as gpt_file:
        json.dump(gpt_data, gpt_file, indent=4)

