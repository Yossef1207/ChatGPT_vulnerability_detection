import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.probability import FreqDist
import json
from tqdm import tqdm
import pyfiglet

filtered_file = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/filtered_data.json"
gpt_json_output = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_1/gpt_output.json"
most_common_words_output = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_1/most_common_words_output.txt"

with open(filtered_file, "r") as f:
    filtered_data = json.load(f)

with open(gpt_json_output, "r") as f:
    data = json.load(f)

count = 1
limit = 50

with open(most_common_words_output, "w") as file:
    for d in data:
        text1 = d['gpt_review_before']
        text2 = d['gpt_review_after']

        words1 = word_tokenize(text1)
        words2 = word_tokenize(text2)
        stop_words = set(stopwords.words('english'))
        lemmatizer = WordNetLemmatizer()
        filtered_words1 = [lemmatizer.lemmatize(word.lower()) for word in words1 if word.lower() not in stop_words and word.isalnum()]
        filtered_words2 = [lemmatizer.lemmatize(word.lower()) for word in words2 if word.lower() not in stop_words and word.isalnum()]
        fdist1 = FreqDist(filtered_words1)
        fdist2 = FreqDist(filtered_words2)

        file.write(pyfiglet.figlet_format('case number: ' + str(count), font="contessa"))
        file.write(f"cwe_id: {d['cwe_id']} \n")
        file.write(f"cwe_name: {filtered_data[count-1]['cwe_name']} \n")
        file.write(f"vuln_id: {filtered_data[count-1]['vuln_id']} \n")
        file.write("vulnerable: \n")
        for word, freq in fdist1.most_common(10):
            file.write(f"{word}: {freq}\n")

        file.write("\nfixed: \n")
        for word, freq in fdist2.most_common(10):
            file.write(f"{word}: {freq}\n")
        count += 1
        if count > limit:
            break

print(count)