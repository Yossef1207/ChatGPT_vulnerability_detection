import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
import json
from nltk.stem import WordNetLemmatizer
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from tqdm import tqdm


def tf_idf(text1, text2):
    documents = [text1, text2] # nltk.sent_tokenize(text)
    stopwords = nltk.corpus.stopwords.words('english')
    lemmatizer = WordNetLemmatizer()

    tokenized_documents = []
    for sentence in documents:
        doc = re.sub('[^a-zA-Z]', ' ', sentence)
        doc = doc.lower()
        doc = doc.split()
        doc = [lemmatizer.lemmatize(word) for word in doc if not word in set(stopwords)]
        doc = ' '.join(doc)
        tokenized_documents.append(doc)

    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(tokenized_documents)

    tf_idf = pd.DataFrame(vectors.todense())
    tf_idf.columns = vectorizer.get_feature_names_out()
    tfidf_matrix = tf_idf.T
    tfidf_matrix.columns = ['fixed_case', 'vulnerable_case']

    tfidf_matrix['count'] = tfidf_matrix.sum(axis=1)

    tfidf_matrix = tfidf_matrix.sort_values(by='count', ascending=False)[:20]
    tfidf_matrix = tfidf_matrix.drop(columns=['count']).head(20)
    return tfidf_matrix




tf_idf_file = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_2/tf_idf_results.txt"
gpt_json_output = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_2/gpt_output.json"

with open(gpt_json_output, "r") as f:
    data = json.load(f)

# Define the set of documents
text1 = data[0]['gpt_review_before']
text2 = data[0]['gpt_review_after']
tfidf_matrix = tf_idf(text1, text2)
print(str(tfidf_matrix))

tfidf_dict = tfidf_matrix.to_dict()

with open('/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_2/tfidf.json', 'w') as f:
    json.dump(tfidf_dict, f, indent=4)
"""
count = 1
limit = 50
with open(tf_idf_file, "w") as file:
    with tqdm(total=limit) as p:
        for d in data:
            file.write("\n\n\n\ncase " + str(count) + ": vulnerable: \n" + d['gpt_review_before'] + "\n\n")
            #print(tf_idf(d['gpt_review_before']))
            file.write("case " + str(count) + ": fixed: \n" + d['gpt_review_after'] + "\n\n\n")
            file.write("TF-IDF matrix: \n" )
            file.write(str(tf_idf(d['gpt_review_after'], d['gpt_review_before'])))
            p.update(1)
            count += 1
            if count > limit:
                break
"""
"""tokens = nltk.word_tokenize(doc)
    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token.isalpha()]
    tokenized_documents.append(' '.join(filtered_tokens))"""
"""
# Tokenize the documents and remove stop words



# Compute the TF-IDF matrix
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(tokenized_documents)

# Print the matrix
print("Word\t", "\t".join([f"Sentence {i+1}" for i in range(len(documents))]))
feature_names = vectorizer.get_feature_names()
for i, doc in enumerate(documents):
    print(f"Sentence {i+1}\t", end='')
    for j in range(len(feature_names)):
        print(f"{tfidf_matrix[i,j]:.2f}\t\t", end='')
    print()

"""
# calculating the sentence similarity using SpaCy
"""vul_sentences = nltk.sent_tokenize(vulnerable_text)
    fix_sentences = nltk.sent_tokenize(fixed_text)
    for j in tqdm(range(len(vul_sentences)), desc='sentence similarity', leave=False):
        sen_count_1 = 1
        sen_count_2 = 1
        mean_sentence_similarity = 0
        for sentence_vul in vul_sentences:
            for sentence_fix in fix_sentences:
                mean_sentence_similarity += nlp(sentence_vul).similarity(nlp(sentence_fix))
                sen_count_2 += 1
            sen_count_1 += 1
        num_sentences = len(vul_sentences) * len(fix_sentences)
        mean_sentence_similarity = mean_sentence_similarity / float(num_sentences)
        pass
    d['sentences_similarity'] = str(mean_sentence_similarity)"""

"""sent1 = nltk.pos_tag(nltk.word_tokenize(vulnerable_text))
    sent2 = nltk.pos_tag(nltk.word_tokenize(fixed_text))
    lemmatizer = WordNetLemmatizer()
    similarity = 0
    for j in tqdm(range(len(sent1)), desc='word similarity', leave=False):
        for word1, pos1 in sent1:
            for word2, pos2 in sent2:
                if pos1 == pos2:
                    synsets1 = wn.synsets(lemmatizer.lemmatize(word1.lower()), pos=pos1)
                    synsets2 = wn.synsets(lemmatizer.lemmatize(word2.lower()), pos=pos2)
                    if synsets1 and synsets2:
                        sim = max(s1.path_similarity(s2) for s1 in synsets1 for s2 in synsets2)
                        similarity += sim
        similarity /= len(sent1)
        pass
    d['word similarity'] = str(similarity)"""
