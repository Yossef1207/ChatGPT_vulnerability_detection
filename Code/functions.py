import csv
from collections import defaultdict

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score
from tqdm import tqdm
import nltk
from nltk.stem import WordNetLemmatizer
import re
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import textwrap
import open_ai_related_functions
import numpy as np
import json
import operator


def general_tf_idf(analysis_json):
    with open(analysis_json, "r") as file:
        data = json.load(file)
    count = 1
    gpt_code_review_vul = []
    gpt_code_review_fix = []

    sorted_data = sorted(data, key=operator.itemgetter('cwe_id'))
    for d in sorted_data:
        gpt_code_review_vul.append(d['gpt_review_before'])
        gpt_code_review_fix.append(d['gpt_review_after'])
    tf_idf(None, None, gpt_code_review_vul)
    tf_idf(None, None, gpt_code_review_fix)


def tf_idf(text1, text2, corpus, documents=True):
    if documents:
        documents = corpus
    else:
        documents = [text1, text2]  # nltk.sent_tokenize(text)

    stopwords = nltk.corpus.stopwords.words('english')
    lemmatizer = WordNetLemmatizer()

    tokenized_documents = []
    for sentence in documents:
        doc = re.sub('[^a-zA-Z]', ' ', sentence)
        doc = doc.lower()
        doc = doc.split()
        doc = [lemmatizer.lemmatize(word) for word in doc if not word in set(stopwords)]
        doc = ' '.join(doc)
        tokenized_documents.append(doc)

    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(tokenized_documents)

    tf_idf = pd.DataFrame(vectors.todense())
    tf_idf.columns = vectorizer.get_feature_names_out()
    tfidf_matrix = tf_idf.T
    if documents:
        tfidf_matrix.columns = [f"Case {i}: " for i in range(1, 51)]
    else:
        tfidf_matrix.columns = ['vulnerable_case', 'fixed_case']

    tfidf_matrix['count'] = tfidf_matrix.sum(axis=1)

    tfidf_matrix = tfidf_matrix.sort_values(by='count', ascending=False)[:20]
    tfidf_matrix = tfidf_matrix.drop(columns=['count']).head(20)

    # print(tfidf_matrix.to_string(index=True, header=True, justify='left'))
    #print(tfidf_matrix.iloc[:, 0])
    row_sums = tfidf_matrix.sum(axis=1)
    print(row_sums)
    return tfidf_matrix


def num_tokens_prompt(code):
    message = [
        {"role": "system", "content": "You a helpful assistent with analyzing Java code snippets"},
        {"role": "system", "content": "You are a helpful assistent that detects security vulnerabilities"},
        {"role": "user",
         "content": f"provide a detailed code review of the following code snippet, and focus on security aspects:\n\n{code}\n\nResponse:"}
    ]
    num_tokens = open_ai_related_functions.num_tokens_from_messages(message)
    return num_tokens


def char_count(string):
    count = 0
    for char in string:
        if char != ' ':
            count += 1
    return count


def word_count_without_stopword_count(text, stop_words):
    words = re.sub('[^a-zA-Z]', ' ', text)
    words = nltk.word_tokenize(words)
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return filtered_words


def similar_sentences(vulnerable_text, fixed_text):
    vul_sentences = nltk.sent_tokenize(vulnerable_text)
    fix_sentences = nltk.sent_tokenize(fixed_text)
    count_similar_sentences = 0
    for sentence_vul in vul_sentences:
        for sentence_fix in fix_sentences:
            similarity = nlp(sentence_vul).similarity(nlp(sentence_fix))
            if similarity >= 0.99:
                count_similar_sentences += 1
                break
    return count_similar_sentences


def text_similarity(vulnerable_text, fixed_text):
    nlp = spacy.load('en_core_web_lg')
    return nlp(vulnerable_text).similarity(nlp(fixed_text))


def write_analysis_text(analysis_json_file, output_file, limit, count=1):

    with open(analysis_json_file, "r") as f:
        data = json.load(f)

    with open(output_file, "w") as file:
        with tqdm(total=limit) as pbar:
            for d in data:
                file.write('Case: ' + str(count) + '\n\n')
                file.write('cwe_id: ' + d['cwe_id'] + '\n\n')
                file.write('cwe_name: ' + d['cwe_name'] + '\n\n')
                file.write('vuln_id: ' + d['vuln_id'] + '\n\n')
                file.write('desc: ' + d['desc'] + '\n\n')

                file.write(
                    '\n----------- This is the vulnerable version ' + ' ( case: ' + str(count) + ') -----------\n\n')
                formatted_string = textwrap.dedent(d['method_before']).strip()
                file.write(formatted_string + '\n\n')
                formatted_string = textwrap.dedent(d['gpt_review_before']).strip()
                file.write(formatted_string + '\n\n')
                file.write('Vulnerability flag: ' + d['cwe_hint_found_vul'] + '\n\n')
                file.write('Word count = ' + d['word_count_vul'] + '\n')
                file.write('Word count without stopwords = ' + d['no_stopwords_word_count_vul'] + '\n')
                file.write('Sentence count = ' + d['sentence_count_vul'] + '\n')
                file.write('Readability score = ' + d['readability_score_vul'] + '\n')
                file.write('The size of the code = ' + d['code_size_vul'] + '\n\n')

                file.write('\n----------- This is the fixed version ' + ' ( case: ' + str(count) + ') -----------\n\n')
                formatted_string = textwrap.dedent(d['method_after']).strip()
                file.write(formatted_string + '\n\n')
                formatted_string = textwrap.dedent(d['gpt_review_after']).strip()
                file.write(formatted_string + '\n\n')
                file.write('Vulnerability flag: ' + d['cwe_hint_found_fix'] + '\n\n')
                file.write('Word count = ' + d['word_count_fix'] + '\n')
                file.write('Word count without stopwords = ' + d['no_stopwords_word_count_fix'] + '\n')
                file.write('Sentence count = ' + d['sentence_count_fix'] + '\n')
                file.write('Readability score = ' + d['readability_score_fix'] + '\n')
                file.write('The size of the code = ' + d['code_size_fix'] + '\n\n')

                file.write('Text Similarity = ' + d['text_similarity'] + '\n\n')
                file.write('Character difference = ' + d['char_diff'] + '\n\n')
                file.write('Number of similar sentences = ' + d['similar_sentences'] + '\n\n')
                file.write('TF_IDF matrix = \n')
                file.write(str(tf_idf(str(d['gpt_review_before']), str(d['gpt_review_after']))) + '\n\n')
                file.write('\n\n\n\n')

                count += 1
                pbar.update(1)
                if count > limit:
                    break






def create_vulnerability_dict(analysis_json, vul):
    with open(analysis_json, "r") as file:
        data = json.load(file)
    vulnerability_dict = {}
    for item in data:
        vulnerability = item['cwe_id']
        label = item['cwe_hint_found_vul' if vul else 'cwe_hint_found_fix']
        if vulnerability in vulnerability_dict:
            vulnerability_dict[vulnerability].append(label)
        else:
            vulnerability_dict[vulnerability] = [label]
    return vulnerability_dict


def statistic(analysis_json, vul):
    flags = create_vulnerability_dict(analysis_json, vul)
    yes_counts = {}
    no_counts = {}

    for key, values in flags.items():
        yes_counts[key] = values.count('Yes')
        no_counts[key] = values.count('No')

    keys = flags.keys()
    x = range(len(keys))

    plt.barh(x, list(yes_counts.values()), height=0.4, label='vulnerable')
    plt.barh(x, list(no_counts.values()), height=0.4, left=list(yes_counts.values()), label='not vulnerable')
    plt.ylabel('Vulnerabilities')
    plt.xlabel('Occurrence')
    plt.title("Prediction plot for the vulnerable set" if vul else "Prediction plot for the fixed set")
    plt.yticks(x, keys)
    plt.legend()
    plt.show()


def metrics(result_csv, CWE=True):
    if CWE:
        a = 4
        b = 6
    else:
        a = 5
        b = 7

    vul_flags_dic = {}
    fix_flags_dic = {}

    vul_list = []
    fix_list = []

    count = 0
    with open(result_csv, "r") as f:
        reader = csv.reader(f, delimiter=',', quotechar='"')
        for row in reader:
            if count > 1:
                row = row[0].split(";")
                """if row[1] in vul_flags_dic:
                    vul_flags_dic[row[1]].append(str(row[a]))
                else:
                    vul_flags_dic[row[1]] = [row[a]]
                if row[1] in fix_flags_dic:
                    fix_flags_dic[row[1]].append(str(row[b]))
                else:
                    fix_flags_dic[row[1]] = [row[b]]"""
                vul_list.append(row[a])
                fix_list.append(row[b])
            count += 1
    print(vul_list)
    print(fix_list)
    gpt_res = vul_list + fix_list
    gpt_res_binary = [1 if label == 'Yes' else 0 for label in gpt_res]
    dataset = [1] * 50 + [0] * 50

    print("\n\nPrompt: provide a detailed code review of the following code snippet, and focus on security aspects:\n")
    print("Confusion matrix = \n", confusion_matrix(dataset, gpt_res_binary, labels=[1, 0]))
    print("Accuracy = ", accuracy_score(dataset, gpt_res_binary))
    print("Precision =", precision_score(dataset, gpt_res_binary, pos_label=1))
    print("Recall = ", recall_score(dataset, gpt_res_binary, pos_label=1))
    print("F1_Score = ", f1_score(dataset, gpt_res_binary, pos_label=1))


def get_list(result_csv, vul_fix):  # this list gives me the Yes and No in a list
    if vul_fix:
        c = 4
    else:
        c = 6
    y_n_lst = []
    count = 0
    with open(result_csv, "r") as f:
        reader = csv.reader(f, delimiter=',', quotechar='"')
        for row in reader:
            row = row[0].split(";")
            if count > 1:
                y_n_lst.append(str(row[c]))
            count += 1
    return y_n_lst


def text_similarity(vulnerable_text, fixed_text):
    nlp = spacy.load('en_core_web_lg')
    return nlp(vulnerable_text).similarity(nlp(fixed_text))


def print_latex_table(analysis_json):
    with open(analysis_json, "r") as file:
        data = json.load(file)
    sorted_data = sorted(data, key=operator.itemgetter('cwe_id'))
    for d in sorted_data:
        print(
            d['case'] + " & " + d['cwe_id'] + " & " + d['cwe_hint_found_vul'] + " & " + d['word_count_vul'] + " & " + d[
                'code_size_vul'] + " & " + d['cwe_hint_found_fix'] + " & " + d['word_count_fix'] + " & " + d[
                'code_size_fix'] + " & " + d['char_diff'] + " & " + "{:.3f}".format(
                float(d['text_similarity'])) + " & " + d['similar_sentences'] + ' \\\\ ' + "\hline")


def calc_statistics(analysis_json, vul):
    true_pred = []
    false_pred = []
    word_count = []
    character = []
    text_sim = []
    char_diff_add = []
    char_diff_sub = []
    sim_sen = []
    with open(analysis_json, "r") as file:
        data = json.load(file)

    sorted_data = sorted(data, key=operator.itemgetter('cwe_id'))
    for d in sorted_data:
        if vul:
            vul = d['cwe_hint_found_vul']
            item_vul = int(d['char_diff'])
            if vul == 'Yes':  # true prediction
                true_pred.append(item_vul)
            if vul == 'No':  # false negative
                false_pred.append(item_vul)
        else:
            fix = d['cwe_hint_found_fix']
            item_fix = int(d['char_diff'])
            if fix == 'No':  # true prediction
                true_pred.append(item_fix)
            if fix == 'Yes':  # false negative
                false_pred.append(item_fix)
        """word_count.append(int(d['word_count_vul']) if vul else int(d['word_count_fix']))
        character.append(int(d['code_size_vul']) if vul else int(d['code_size_fix']))
        text_sim.append(float(d['text_similarity']))
        if int(d['char_diff']) >= 0:
            char_diff_add.append(int(d['char_diff']) if vul else int(d['char_diff']))
        else:
            char_diff_sub.append(int(d['char_diff']) if vul else int(d['char_diff']))
        sim_sen.append(int(d['similar_sentences']) if vul else int(d['similar_sentences']))
        """

    print(f"True positive mean = {np.mean(true_pred)}")
    print(f"True positive std = {np.std(true_pred)}\n")
    print(f"False negative mean = {np.mean(false_pred)}")
    print(f"False negative std = {np.std(false_pred)}\n")
    """print(f"Max word count = {max(word_count)}")
    print(f"Min word count = {min(word_count)}")
    print(f"Mean word count = {np.mean(word_count)}")
    print(f"STD word count = {np.std(word_count)}")
    print(f"Max char  = {max(character)}")
    print(f"Min char  = {min(character)}")
    print(f"Mean char  = {np.mean(character)}")
    print(f"STD char  = {np.std(character)}")
    print(f"Max Text similarity = {max(text_sim)}")
    print(f"Min Text similarity = {min(text_sim)}")
    print(f"Mean Text similarity = {np.mean(text_sim)}")
    print(f"STD Text similarity = {np.std(text_sim)}")
    print(f"Mean char_diff add = {np.mean(char_diff_add)}")
    print(f"STD char_diff add = {np.std(char_diff_add)}")
    print(f"Mean char_diff sub = {np.mean(char_diff_sub)}")
    print(f"STD char_diff sub = {np.std(char_diff_sub)}")
    print(f"Mean sim_sen sub = {np.mean(sim_sen)}")
    print(f"STD sim_sen sub = {np.std(sim_sen)}")"""


# this gives me the CWE with their count in the document in a dictionary
def CWE_count(analysis_json):
    with open(analysis_json, "r") as file:
        data = json.load(file)
    cwe_counts = defaultdict(int)
    for d in data:
        cwe_id = d['cwe_id']
        cwe_counts[cwe_id] += 1
    # Print the counts
    for cwe_id, count in cwe_counts.items():
        print(f"CWE ID: {cwe_id}, Count: {count}")
