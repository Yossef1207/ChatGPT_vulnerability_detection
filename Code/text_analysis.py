import sys

import textstat as textstat
from tqdm import tqdm
import open_ai_related_functions
import spacy
import nltk
import json
from nltk.stem import WordNetLemmatizer
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import csv
import functions





if __name__ == '__main__':

    """args = sys.argv
    directory = args[1]
    if len(args) == 2 and (int(args[1]) == 1 or int(args[1]) == 2):
        gpt_json_output = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_" + directory + "/gpt_output.json"
        analysis_output = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_" + directory + "/analysis_output.json"
        result_csv = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_" + directory + "/part_result.csv"
    else:
        print("invalid input")
        sys.exit(1)"""
    for directory_num in range(1, 3):
        directory = str(directory_num)
        gpt_json_output = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_" + directory + "/gpt_output.json"
        analysis_output = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_" + directory + "/analysis_output.json"
        result_csv = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_" + directory + "/part_result.csv"
        combined_txt = "/Users/yousefalbunni/Desktop/bsc-yossef-al-buni/Code/output_" + directory + "/combined_result.txt"
        vul_flags = []
        fix_flags = []
        count = 0
        with open(result_csv, "r") as f:
            reader = csv.reader(f, delimiter=',', quotechar='"')
            for row in reader:
                if count > 0:
                    row = row[0].split(";")
                    vul_flags.append(row[4])
                    fix_flags.append(row[6])
                count += 1

        with open(gpt_json_output, "r") as f:
            data = json.load(f)
        stopwords = nltk.corpus.stopwords.words('english')

        count = 1
        limit = 50
        with tqdm(total=limit) as pbar:
            for d in data:
                vulnerable_text = d['gpt_review_before']
                fixed_text = d['gpt_review_after']

                d['case'] = str(count)

                d['word_count_vul'] = str(len(nltk.word_tokenize(vulnerable_text)))
                d['no_stopwords_word_count_vul'] = str(len(functions.word_count_without_stopword_count(vulnerable_text, stopwords)))
                d['sentence_count_vul'] = str(len(nltk.sent_tokenize(vulnerable_text)))
                d['readability_score_vul'] = str(textstat.flesch_reading_ease(vulnerable_text))
                vul_code_size = functions.char_count(d['method_before'])
                d['code_size_vul'] = str(vul_code_size)
                # d['prompt_tok_vul'] = str(functions.num_tokens_prompt(d['method_before']))
                d['cwe_hint_found_vul'] = str(vul_flags[count])

                d['word_count_fix'] = str(len(nltk.word_tokenize(fixed_text)))
                d['no_stopwords_word_count_fix'] = str(len(functions.word_count_without_stopword_count(fixed_text, stopwords)))
                d['sentence_count_fix'] = str(len(nltk.sent_tokenize(fixed_text)))
                d['readability_score_fix'] = str(textstat.flesch_reading_ease(fixed_text))
                fix_code_size = functions.char_count(d['method_after'])
                d['code_size_fix'] = str(fix_code_size)
                # d['prompt_tok_fix'] = str(functions.num_tokens_prompt(d['method_after']))
                d['cwe_hint_found_fix'] = str(fix_flags[count])

                d['text_similarity'] = str(functions.text_similarity(vulnerable_text, fixed_text))
                d['char_diff'] = str(fix_code_size - vul_code_size)
                d['similar_sentences'] = str(functions.similar_sentences(vulnerable_text, fixed_text))
                d['TF-IDF'] = str(functions.tf_idf(vulnerable_text, fixed_text, None))

                count += 1
                pbar.update(1)
                if count > limit:
                    break

        with open(analysis_output, "w") as analysis_file:
            json.dump(data[:limit], analysis_file, indent=4)

        functions.write_analysis_text(analysis_output, combined_txt, limit, 1)
